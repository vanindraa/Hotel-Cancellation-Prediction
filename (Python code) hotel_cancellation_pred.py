# -*- coding: utf-8 -*-
"""Hotel Cancellation Pred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H25J-AHlsZSgw7_CBGjV2Xla8b0Fr7Nh

# DATA DESC
The online hotel reservation channels have dramatically changed booking possibilities and customersâ€™ behavior. A significant number of hotel reservations are called-off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with.

Can you predict if the customer is going to honor the reservation or cancel it ?

https://www.kaggle.com/datasets/ahsan81/hotel-reservations-classification-dataset

# DATA DICTIONARY
The file contains the different attributes of customers' reservation details. The detailed data dictionary is given below.

Data Dictionary:

- Booking_ID: unique identifier of each booking - Drop
- no_of_adults: Number of adults
- no_of_children: Number of Children
- no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel
- no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel
- type_of_meal_plan: Type of meal plan booked by the customer:
- required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)
- room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.
- lead_time: Number of days between the date of booking and the arrival date
- arrival_year: Year of arrival date
- arrival_month: Month of arrival date
- arrival_date: Date of the month
- market_segment_type: Market segment designation.
- repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)
- no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking
- no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking
- avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)
- no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)
- booking_status: Flag indicating if the booking was canceled or not. -> later is turned into 1 and 0
    - 1 = cancelled
    - 0 = not cancelled

# GOAL
To predict which customers will honor their reservation (would not cancel) - Should be supervised, we can use Decision Tree or Random Forest Classifier
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

#Size for graphs/visualization used
plt.rcParams["figure.figsize"] = [20, 20]
plt.rcParams["figure.autolayout"] = True

data = pd.read_csv('Hotel Reservations.csv')
data.head()

data.info()

"""I am Dropping the Booking_ID because it would not be included in machine learning. other than that, it does not provide additional insights"""

# Checking for missing values
data.isnull().sum()

"""No indication of missing values in this dataset

# 1. Data Profiling

## Descriptive Profiling
"""

# Distribution of each features
data.hist()
plt.show()

data.describe()

"""Average pricing for the room can not be 0. As the data suggested, the minimum number is 0, therefore we need to treat it as a missing value"""

# Finding and treating missing values
data.loc[data['avg_price_per_room']==0]

"""There are a total of 545 rows with 0 as a value in avg price room. I think it is safe to just fill it out mith median"""

data['avg_price_per_room'] = data['avg_price_per_room'].replace([0],data['avg_price_per_room'].median())

data['avg_price_per_room'].describe()

data['avg_price_per_room'].hist()
plt.show()

"""Setelah diisi dengan median, rata-rata dan standard deviasi tidak berubah jauh. Selain itu, persebarannya juga tidak berubah, tetap condong skewed ke kanan dan berkemungkinan menunjukkan outliers."""

#Object data profiling
data.describe(include='O')

"""## Checking Duplicated Data"""

data[data.duplicated()]

"""There are no duplicated values. It is very much possible for all the features to have duplicates except the Booking_ID. from here, we can conclude that there is no cuplicated values.

## Checking if the data is balance or not
"""

data.groupby('booking_status').size()

data['booking_status'].value_counts(normalize=True)

"""Data yang diberikan memiliki ketimpangan 67 banding 33 persen. Ini menunjukkan data imbalance sehingga akan dilakukan oversampling dengan SMOTE saat merancang model Machine Learning.

## Simplifying Features Name
"""

# Before renaming features
data.info()

data.rename(columns={'no_of_adults':'adults'},inplace=True)
data.rename(columns={'no_of_children':'children'},inplace=True)
data.rename(columns={'no_of_weekend_nights':'weekend_nights'},inplace=True)
data.rename(columns={'no_of_week_nights':'weekday_nights'},inplace=True)
data.rename(columns={'type_of_meal_plan':'meal_plan'},inplace=True)
data.rename(columns={'no_of_weekend_nights':'weekend_nights'},inplace=True)
data.rename(columns={'no_of_special_requests':'special_requests'},inplace=True)
data.rename(columns={'market_segment_type':'market_type'},inplace=True)
data.rename(columns={'special_requests':'request_num'},inplace=True)

data.rename(columns={'required_car_parking_space':'parking_space'},inplace=True)

data.rename(columns={'room_type_reserved':'room_type'},inplace=True)
data.rename(columns={'avg_price_per_room':'avg_room_price'},inplace=True)

#after renaming
data.info()

"""## Finding Outliers"""

data.describe()

plt.rcParams["figure.figsize"] = [8, 8]
plt.rcParams["figure.autolayout"] = True

plt.boxplot(data['avg_room_price'])
plt.show()

Q1 = np.percentile(data.avg_room_price,25)
Q3 = np.percentile(data.avg_room_price,75)
IQR = (Q3-Q1)
print("IQR sama dengan", IQR)

Lbound = Q1-(IQR*1.5)
Ubound = Q3+(IQR*1.5)
print('Lower Bound =',Lbound)
print('Upper Bound =',Ubound)

data.loc[(data['avg_room_price']<Lbound)]

"""Outliers from the avg_room_price feature is not going to be removed because it will delete many informations and room price is dynamic and very much possible to have a big or low average on this"""

data.head()

"""# MAKING THE TARGET NUMERICAL"""

data['booking_status'].unique()

data['booking_status'] = data['booking_status'].replace(['Not_Canceled','Canceled'],[0,1])

data.head()

data.describe()

"""# EDA"""

data.info()

cancel = data.loc[data['booking_status']==1]
nocancel = data.loc[data['booking_status']==0]

"""## Total Cancellation"""

total = data.groupby('booking_status')[['adults']].count().reset_index()
total

plt.bar(total['booking_status'],total['adults'])
plt.show

"""## How Many Adults Cancel/not"""

#Number of adults brought that cancel their reservation
adult = cancel.groupby('adults')[['booking_status']].count().reset_index()
adult

#Number of adults brought that cancel their reservation
sns.barplot(adult,x='adults',y='booking_status')

# Number of adults brought in which they don't cancel the reservation
adult2 = nocancel.groupby('adults')[['booking_status']].count().reset_index()
adult2

# Number of adults brought in which they don't cancel the reservation
sns.barplot(adult2,x='adults',y='booking_status')

g=sns.kdeplot(data['adults'][data['booking_status']==0], color='Red',shade=True)
g=sns.kdeplot(data['adults'][data['booking_status']==1], color='Green',shade=True)
g.set_xlabel('adults')
g.set_ylabel('frequency')
g.legend(['Not Cancelled','Cancelled'])

sns.jointplot(x='adults', y='booking_status', data=data, kind="reg")

"""Key Takeaway: Kebanyakan tamu yang datang ke hotel berjumlah dua orang dewasa. Asumsi yang muncul adalah kebanyakan merupakan pasangan. Perbandingan antara yang membatalkan dan tidak membatalkan booking cukup seimbang di masing-masing angka orang dewasa. Namun, menariknya justru saat hanya satu orang dewasa, lebih cenderung menahan reservasi ketimbang cancel"""



"""## Number of child"""

#number of children brought but not cancel
child = nocancel.groupby('children')[['booking_status']].count().reset_index()
child

#number of children brought but not cancel
sns.barplot(child,x='children',y='booking_status')

#Number of child brought but cancelled the reservation
canchild = cancel.groupby('children')[['booking_status']].count().reset_index()
canchild

#Number of child brought but cancelled the reservation
sns.barplot(canchild,x='children',y='booking_status')

g=sns.kdeplot(data['children'][data['booking_status']==0], color='Red',shade=True)
g=sns.kdeplot(data['children'][data['booking_status']==1], color='Green',shade=True)
g.set_xlabel('adults')
g.set_ylabel('frequency')
g.legend(['Not Cancelled','Cancelled'])

"""Key takeaway: People who don't bring children is less likely to cancel"""

sns.jointplot(x='children', y='booking_status', data=data, kind="reg")

"""## Weekend or Weekday?"""

mix = data.loc[(data['weekday_nights']>0) & (data['weekend_nights']>0)]
weekday = data.loc[(data['weekday_nights']>0) & (data['weekend_nights']==0)]
weekend = data.loc[(data['weekend_nights']>0) & (data['weekday_nights']==0)]

mix = pd.Series(len(mix))
weekday = pd.Series(len(weekday))
weekend = pd.Series(len(weekend))

day = pd.concat([mix,weekday,weekend],axis=0)
day = pd.DataFrame(day).reset_index()
day.drop(columns='index',axis=1,inplace=True)
day

day.rename(columns={0:'Count'},inplace=True)
day.rename(index={0:'mix'},inplace=True)
day.rename(index={1:'weekday'},inplace=True)
day.rename(index={2:'weekend'},inplace=True)

day = day.reset_index()
day.rename(columns={'index':'Type'},inplace=True)
day

sns.barplot(day,x='Type',y='Count')

"""Key Takeaway: As we can see, people tend to book a hotel in a weekday going straight to the weekend (mix) instead just for the weekend. Other than that, it is surprisingly low how people stay in a hotel for just the weekend. Instead, they tend to stay during the weekday. However, this should be taken with a grain of salt as context is very much important. Most of them could just be on a business trip, or the data was taken during the holiday. We need more insight regarding this."""

mix = data.loc[(data['weekday_nights']>0) & (data['weekend_nights']>0)]
weekday = data.loc[(data['weekday_nights']>0) & (data['weekend_nights']==0)]
weekend = data.loc[(data['weekend_nights']>0) & (data['weekday_nights']==0)]

"""# FEATURE CORRELATION"""

corr = data.corr()
corr

plt.rcParams["figure.figsize"] = [20, 20]
plt.rcParams["figure.autolayout"] = True

sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,
            annot=True)

"""# FEATURE SCALLING AND ENCODING"""

#Importing Scaler and Encoder
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OneHotEncoder

data['meal_plan'].unique()

data['room_type'].unique()

rscaler = RobustScaler()

ohc = OneHotEncoder(handle_unknown = 'ignore')

data.drop(columns=['Booking_ID'], axis=1, inplace=True)

data.head()

"""Booking ID is dropped because it will not be used"""

# Copying the dataset for scalling
data_scaled = data.copy()

data_scaled.head()

# SCalling the numerical features
data_scaled['adults'] = rscaler.fit_transform(data_scaled[['adults']])
data_scaled['children'] = rscaler.fit_transform(data_scaled[['children']])
data_scaled['weekend_nights'] = rscaler.fit_transform(data_scaled[['weekend_nights']])
data_scaled['weekday_nights'] = rscaler.fit_transform(data_scaled[['weekday_nights']])
data_scaled['lead_time'] = rscaler.fit_transform(data_scaled[['lead_time']])
data_scaled['no_of_previous_cancellations'] = rscaler.fit_transform(data_scaled[['no_of_previous_cancellations']])
data_scaled['no_of_previous_bookings_not_canceled'] = rscaler.fit_transform(data_scaled[['no_of_previous_bookings_not_canceled']])
data_scaled['avg_room_price'] = rscaler.fit_transform(data_scaled[['avg_room_price']])
data_scaled['request_num'] = rscaler.fit_transform(data_scaled[['request_num']])

data_scaled.head()

"""Numerical data is already scaled, now is time to transform categorical data using One Hot Encoder"""

col = sorted(data['meal_plan'].unique().tolist()) + sorted(data['room_type'].unique().tolist()) + sorted(data['market_type'].unique().tolist())

#Encoding
enc_df = pd.DataFrame(ohc.fit_transform(data_scaled[['meal_plan', 'room_type', 'market_type']]).toarray(), columns=col)

enc_df.head()

#Joining the scaled dataframe with encoded dataframe
data_scaled = data_scaled.join(enc_df)
data_scaled.head()

# Dropping the unused features
data_scaled.drop(columns=['meal_plan','room_type','market_type', 'arrival_year','arrival_month','arrival_date'],axis=1,inplace=True)

# joining dummies to the dataframe
final_data=data_scaled.copy()

final_data.info()

"""All the features are scaled, except for the categorical 1 and 0 ones like parking space, repeated guest, and booking status

# SPLITTING X Y AND BALANCING DATA
"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from collections import Counter

#Splitting X and Y
X = final_data.drop(columns=['booking_status'],axis=1)
y = final_data['booking_status']

#split train-rest data
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)

X_train.shape

y_train.shape

y_train.value_counts()

print('Before Oversampling: ',Counter(y_train))
#defining smote
SMOTE = SMOTE(random_state=0)

#fit and apply the transform
X_train_smote, y_train_smote = SMOTE.fit_resample(X_train,y_train)

#summarize class distribution
print('After Oversampling: ',Counter(y_train_smote))

"""# MACHINE LEARNING MODELING"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

"""## Decision Tree"""

# Decision Tree without resampling
dt = DecisionTreeClassifier(random_state=0)
dt.fit(X_train, y_train)

# Decision Tree with resampling
dt2 = DecisionTreeClassifier(random_state=0)
dt2.fit(X_train_smote, y_train_smote)

"""## RANDOM FOREST CLASSIFIER"""

# Random Forest without resampling
rf1 = RandomForestClassifier(random_state=0)
rf1.fit(X_train,y_train)

# Random Forest with resampling
rf2 = RandomForestClassifier(random_state=0)
rf2.fit(X_train_smote,y_train_smote)

"""## KNN"""

from sklearn.neighbors import KNeighborsClassifier

# KNN without resampling
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# KNN with resampling
knn2 = KNeighborsClassifier()
knn2.fit(X_train_smote, y_train_smote)

"""## SVC Linear"""

from sklearn.svm import SVC

# SVCL without resampling
svc = SVC(kernel='linear',random_state=0)
svc.fit(X_train,y_train)

# SVCL with resampling
svc2 = SVC(kernel='linear',random_state=0)
svc2.fit(X_train_smote,y_train_smote)

"""## SVC RBF"""

# SVCR without resampling
svc3 = SVC(kernel='rbf',random_state=0)
svc3.fit(X_train,y_train)

# SVCR with resampling
svc4 = SVC(kernel='rbf',random_state=0)
svc4.fit(X_train_smote,y_train_smote)

"""# MODEL EVALUATION"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, silhouette_score
from sklearn.metrics import classification_report

"""## DT without resampling"""

y_pred_dt = dt.predict(X_test)
print(classification_report(y_test, y_pred_dt))

"""## DT with resampling"""

y_pred_dt2 = dt2.predict(X_test)
print(classification_report(y_test, y_pred_dt2))

"""## Random Forest without resampling"""

y_pred_rf1 = rf1.predict(X_test)
print(classification_report(y_test, y_pred_rf1))

"""## Random Forest With Resampling"""

y_pred_rf2 = rf2.predict(X_test)
print(classification_report(y_test, y_pred_rf2))

"""## KNN No Resampling"""

y_pred_knn = knn.predict(X_test)
print(classification_report(y_test, y_pred_knn))

"""## KNN With Resampling"""

y_pred_knn2 = knn2.predict(X_test)
print(classification_report(y_test, y_pred_knn2))

"""## SVCL No Resampling"""

y_pred_svc = svc.predict(X_test)
print(classification_report(y_test, y_pred_svc))

"""## SVCL With Resampling"""

y_pred_svc2 = svc2.predict(X_test)
print(classification_report(y_test, y_pred_svc2))

"""## SVC RBF No Resampling"""

y_pred_svc3 = svc3.predict(X_test)
print(classification_report(y_test, y_pred_svc3))

"""## SVC RBF With Resampling"""

y_pred_svc4 = svc4.predict(X_test)
print(classification_report(y_test, y_pred_svc4))

"""# CHECKING FEATURE IMPORTANCE"""

importances = rf1.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf1.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()

plt.rcParams["figure.figsize"] = [20, 10]
plt.rcParams["figure.autolayout"] = True

# Checking Feature Importance
features = X.columns
importances = rf1.feature_importances_
indices = np.argsort(importances)[-15:]
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""# RE TRAINING
Training the models with dropping least important features
"""

final_data.info()

"""## SPLITTING X AND Y"""

from imblearn.over_sampling import SMOTE

#Splitting X and Y
# X = final_data['adults','children','lead_time','avg_room_price','request_num','weekday_nights','weekend_nights','Online','Offline']
X = final_data.drop(columns=['parking_space','repeated_guest','no_of_previous_cancellations','no_of_previous_bookings_not_canceled',
                            'Meal Plan 1','Meal Plan 2','Meal Plan 3','Not Selected','Room_Type 1','Room_Type 2','Room_Type 3','Room_Type 4',
                            'Room_Type 5','Room_Type 6','Room_Type 7','Aviation','Complementary','Corporate','booking_status'], axis=1)
y = final_data['booking_status']

#split train-rest data
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)

print('Before Oversampling: ',Counter(y_train))
#defining smote
SMOTE = SMOTE(random_state=0)

#fit and apply the transform
X_train_smote, y_train_smote = SMOTE.fit_resample(X_train,y_train)

#summarize class distribution
print('After Oversampling: ',Counter(y_train_smote))

# Random Forest without resampling
rf3 = RandomForestClassifier(random_state=0)
rf3.fit(X_train,y_train)

y_pred_rf3 = rf3.predict(X_test)
print(classification_report(y_test, y_pred_rf3))

# Random Forest with resampling
rf4 = RandomForestClassifier(random_state=0)
rf4.fit(X_train_smote,y_train_smote)

y_pred_rf4 = rf4.predict(X_test)
print(classification_report(y_test, y_pred_rf4))

"""# HYPER PARAMETER TUNING"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search over
param_grid = {
    'n_estimators': [10, 50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}


# Create the GridSearchCV object
grid_search = GridSearchCV(rf3, param_grid=param_grid, cv=5)

# Fit the GridSearchCV object to the data
grid_search.fit(X, y)

# Print the best hyperparameters and the corresponding score
print("Best hyperparameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)

grid_search.best_params_

grid_search.best_estimator_

best_grid = grid_search.best_estimator_

y_pred_grid = best_grid.predict(X_test)
print(classification_report(y_test, y_pred_grid))